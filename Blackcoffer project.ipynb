{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69f67276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5496421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting input data\n",
    "data=pd.read_excel(r\"D:\\python DSA\\classroom technogeeks\\datasets\\Input.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6d03c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# createing folder to save text files\n",
    "os.makedirs('web_extraction_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a57e9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating func that scrap the website and store all info in text file inside upper folder \n",
    "def func_for_web_extraction():\n",
    "    for i in range(len(data)):\n",
    "        file_path=os.path.join('web_extraction_folder',f\"text_file.no-{data['URL_ID'][i]}\") #creating text file to save article\n",
    "        html_data = requests.get(data['URL'][i]).text # getting html content\n",
    "        soup=BeautifulSoup(html_data,'html.parser') # using Beautiful soup for scraping\n",
    "        try:\n",
    "            heading = soup.find('h1').text  # trying to get article headline\n",
    "        except:\n",
    "            heading=''                   \n",
    "        try:\n",
    "            article=''\n",
    "            for j in soup.find_all('article'): # scraping a whole website\n",
    "                for k in j.find_all('p'):\n",
    "                    article += k.text\n",
    "        except:\n",
    "            article=''\n",
    "        with open(file_path,'w',encoding='utf-8') as a: # saving this text in file\n",
    "            a.write(heading + '\\n' + article)\n",
    "func_for_web_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ae8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of all stopwords of that you gave us and nltk library\n",
    "stopwords_list=['Auditor','Currencies','DatesandNumbers','Generic','GenericLong','Geographic']\n",
    "total_stopwords=stopwords.words('english')\n",
    "for i in stopwords_list:\n",
    "    file_path=os.open(f'StopWords/StopWords_{i}.txt',os.O_RDONLY)\n",
    "    with open(file_path,'r') as a:\n",
    "        for j in a.readlines():\n",
    "            if '|' in j:              \n",
    "                total_stopwords.extend(j.replace('\\n','').replace(' ','').lower().split('|'))\n",
    "            else:\n",
    "                total_stopwords.append(j.replace('\\n','').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f88fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_stopwords=list(set(total_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e715b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of negative and positive words\n",
    "\n",
    "negative_words=[]\n",
    "positive_words=[]\n",
    "for i in ['negative-words','positive-words']:\n",
    "    file_path=os.open(f'MasterDictionary/{i}.txt',os.O_RDONLY)\n",
    "    with open(file_path,'r') as a:\n",
    "        for j in a.readlines():\n",
    "            if i == 'negative-words':\n",
    "                negative_words.append(j.replace('\\n','').lower())    \n",
    "            else:\n",
    "                positive_words.append(j.replace('\\n','').lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c9cc05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating functions that calculate total positive ,negative, polarity,syllable,complex_words,personal pronounce\n",
    "def positive_scor(text):\n",
    "    positive_sco = 0\n",
    "    negative_sco = 0\n",
    "    for i in text:\n",
    "        if i in positive_words:\n",
    "            positive_sco +=1\n",
    "        elif i in negative_words:\n",
    "            negative_sco += -1\n",
    "    \n",
    "    return [positive_sco,negative_sco*-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b298fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_scor(positive_score,negative_score):\n",
    "    return round((positive_score - negative_score)/((positive_score + negative_score)+0.000001),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e9dfbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_complexwords(word_token):\n",
    "    syllable_count = 0\n",
    "    complex_words_count=0\n",
    "    for i in word_token:\n",
    "        if i.endswith('es') or i.endswith('ed'):\n",
    "            word_dict = Counter(i[len(i)-2])\n",
    "            sum_count = sum([word_dict['a'],word_dict['e'],word_dict['i'],word_dict['o'],word_dict['u']])\n",
    "            if sum_count > 2:\n",
    "                complex_words_count += 1\n",
    "                syllable_count += sum_count\n",
    "            else:\n",
    "                syllable_count += sum_count\n",
    "        else:\n",
    "            word_dict = Counter(i)\n",
    "            sum_count = sum([word_dict['a'],word_dict['e'],word_dict['i'],word_dict['o'],word_dict['u']])\n",
    "            if sum_count > 2:\n",
    "                complex_words_count += 1\n",
    "                syllable_count += sum_count\n",
    "            else:\n",
    "                syllable_count += sum_count\n",
    "    return syllable_count,complex_words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6341d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_pronoun(word_token):\n",
    "    word_count = 0\n",
    "    for i in word_token:\n",
    "        if i in ['I','We','my','ours','us']:\n",
    "            word_count += 1\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e3c91155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating main function that they calculate all whole output table for all text files \n",
    "\n",
    "def calc():\n",
    "    output_array = []\n",
    "    for i in range(len(data)):\n",
    "        url = data['URL'][i]\n",
    "        url_id = data['URL_ID'][i]\n",
    "        try:                                                      # try diffrent encoding if this two not works\n",
    "            with open(f\"web_extraction_folder/text_file.no-{data['URL_ID'][i]}\",encoding='utf-8') as a:\n",
    "                text = a.read()\n",
    "        except:\n",
    "            with open(f\"web_extraction_folder/text_file.no-{data['URL_ID'][i]}\",encoding='ansi') as a:\n",
    "                text = a.read()\n",
    "        sent_token=sent_tokenize(text.lower())\n",
    "        text = re.sub('\\s',' ',text.translate(str.maketrans('','',string.punctuation)))\n",
    "        word_token=word_tokenize(text.lower())\n",
    "        personal_pronouns = personal_pronoun(word_tokenize(text))\n",
    "        for i in word_token:\n",
    "            if i in total_stopwords:\n",
    "                word_token.remove(i)\n",
    "            else:\n",
    "                pass\n",
    "        if len(word_token) > 0:   # here we will get all values\n",
    "            positive_score,negative_score = positive_scor(word_token)[0],positive_scor(word_token)[1]\n",
    "            polarity_score = polarity_scor(positive_score,negative_score)\n",
    "            subjectvity_scor = lambda x,y,z:(x+y)/((z)+0.000001)\n",
    "            subjectvity_score = round(subjectvity_scor(positive_score,negative_score,len(word_token)),2)\n",
    "            avg_sent_len = sum([len(i.split(' '))/len(sent_token) for i in sent_token])/len(sent_token)\n",
    "            syllable,complex_words = syllable_complexwords(word_token)[0],syllable_complexwords(word_token)[1]\n",
    "            percent_complex_words = round(complex_words/ len(word_token),2)*100\n",
    "            fog_index = round(0.4 * (avg_sent_len + percent_complex_words),2)\n",
    "            avg_no_words_per_sent = round(len(word_token) / len(sent_token),2)\n",
    "            avg_word = round(sum([len(i) for i in word_token])/len(word_token),2)\n",
    "            word_count=len(word_token)\n",
    "            output_array.append([url_id,url,positive_score,negative_score,polarity_score,subjectvity_score,avg_sent_len,percent_complex_words,\n",
    "                    fog_index,avg_no_words_per_sent,complex_words,word_count,syllable,personal_pronouns,avg_word])\n",
    "        else:             # it is for if any file cant hhave any info\n",
    "            output_array.append([url_id,url,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "52528d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating pandas dataframe\n",
    "output_data = pd.DataFrame(calc(),columns=['URL_ID','URL','POSITIVE_SCORE','NEGATIVE_SCORE','POLARITY_SCORE','SUBJECTIVITY_SCORE','AVG_SENTENCE_LEN','PERCENTAGE_OF_COMPLEX','FOG_INDEX','AVG_NO_OF_WORDS','COMPLEX_WORD_COUNT','WORD_COUNT','SYALLABLE_PER_WORD','PERSONAL_PRONOUNS','AVG_WORD_LENGTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a3b37258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the excel file\n",
    "output_data.to_excel('output_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32aa78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6f9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc4f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4458d5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d1ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
